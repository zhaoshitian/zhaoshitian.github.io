<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Shitian Zhao</title>

    <meta name="author" content="Shitian Zhao">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:80%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Shitian Zhao (赵世天)
                </p>
		<p>Now I am a research intern in Shanghai AI Lab, <a href="https://alpha-vllm.github.io/">Alpha-VLLM team</a>, in my gap year, mentored by <a href="https://gaopengcuhk.github.io/">Peng Gao</a>.
		I got my bachelor degree in the summer of 2024 from <a href="https://english.ecnu.edu.cn/">East China Normal University</a>, supervised by Professor <a href="https://wangyan921.github.io/">Yan Wang</a>. And I was an intern in <a href="https://ccvl.jhu.edu/">CCVL</a>@Johns Hopkins University, supervised by <a href="https://www.cs.jhu.edu/~ayuille/">Bloomberg Distinguished Professor Alan Yuille</a> and <a href="https://lizw14.github.io/">Zhuowan Li</a>.
                </p>
		<p>Email: zhaosh5t5an@gmail.com
		</p>
		<p>Wechat: zstt0135
		</p>
                <p style="text-align:center">
<!--                   <a href="mailto:zhaosh5t5an@gmail.com">Email</a> &nbsp;/&nbsp; -->
                  <a href="https://drive.google.com/file/d/1MSwfJI-ljTeKYOmjzeOqkjf39alfou0o/view?usp=sharing">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=84t2PkoAAAAJ&hl=en">G-Scholar</a> &nbsp;/&nbsp;
		  <a href="https://www.semanticscholar.org/author/Shitian-Zhao/2291854595">S-Scholar</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/zst96687522">Twitter</a> &nbsp;/&nbsp;
		  <a href="https://www.linkedin.com/in/%E4%B8%96%E5%A4%A9-%E8%B5%B5-6481b4213/">LinkedIn</a> &nbsp;/&nbsp;
                  <a href="https://github.com/zhaoshitian/">Github</a> &nbsp;/&nbsp;
		  <a href="https://hub.docker.com/u/zhaoshitian">Dockerhub</a> &nbsp;/&nbsp;
		  <a href="https://huggingface.co/stzhao">HuggingFace</a> &nbsp;/&nbsp;
		  <a href="https://www.zhihu.com/people/bie-lai-wu-yang-39-60">Zhihu(知乎)</a>
                </p>
              </td>
              <td style="padding:0%;width:100%;max-width:100%">
                <a href="images/zst.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/zst.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
              <td style="padding:10px;width:100%;vertical-align:middle">
                  <p>
                    <!-- <span style="background-color: #FFFF00"> -->
                    <font color="orange"><strong>Application</strong></font>: I am currently in the process of applying for a PhD program in Artificial Intelligence for 2025fall. If you are also applying or are interested in talking about research, please do not hesitate to contact me! I am more than happy to connect and share insights.</p>
              </td>
          </tr>
      </tbody></table>
		
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
		<p>
		My Long-Term Research Interests include:
		</p>
		<ul>
		  <li><strong>Multi-modality Generative Models</strong>: Modeling multi-modal content in the generative way, under various paradigms, e.g., autoregressive model, diffusion model, etc.<br><a href="https://arxiv.org/abs/2408.02657">Lumina-mGPT</a>, <a href="https://arxiv.org/abs/2402.05935">SPHINX-X</a></li>
		  <li><strong>Inferece Time Scaling Law</strong>: More computation in the inference time results in higher intellectual level of the AI system.<br><a href="https://arxiv.org/abs/2312.06685">Causal-CoG</a>, <a href="https://arxiv.org/abs/2410.00363">Likelihood Composition</a></li>
		  <li><strong>Compositionality</strong>: Decomposing the images or unstructured text to help reasoning and planning.</li>
		</ul>
<!--                 <p>
                  My interest lies on generative models, with a particular focus on multi-modal language models and diffusion models. I find it more compelling to consider large language models (LLMs) and latent diffusion models (LDMs) from the perspective of energy-based models (EBMs). The essence of generative modeling is to capture the underlying distribution or rules of our world. As such, the inherent knowledge embedded within these models serves as an excellent foundation for downstream tasks.
                </p> -->
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2>News</h2>
              <!-- <p>
                  <li><span style="background-color: #FFFF00"> New </span><font color="orange"><strong>On the job market</strong></font>: I am currently on the job market. I am interested in both industry and academic positions. Don’t hesitate to email me if there is a potential fit.</p> -->
              <ul>
		<li><strong>[Sept. 2024]</strong> Likelihood Composition is accepted by EMNLP 2024 as findings.</li>
		<li><strong>[May 2024]</strong> Give a talk of Causal-CoG at CCVL@JHU's group meeting.</li>
                <li><strong>[April 2024]</strong> Causal-CoG is accepted by CVPR 2024 as Poster (Highlight, top 2.8%).</li>
		<li><strong>[Dec. 2023]</strong> Join Shanghai AI Lab as a research intern.</li>
              </ul>
                <p style="text-align:right;font-size:small;">
                  Last updated: 2024/9/26.
                </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        
        <hr>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2>Publications</h2>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

    <!-- <tr bgcolor="#ffffd0"> -->

    <tr>
      <td style="padding:20px;width:35%;vertical-align:middle">
          <img src='images/PixWizard.jpeg' width="250">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2409.15278">
          <papertitle>PixWizard: Versatile Image-to-Image Visual Assistant with Open-Language Instructions</papertitle>
        </a>
        <br>
	<a>Weifeng Lin*</a>,
	<a>Xinyu Wei*</a>,
	<a>Renrui Zhang*</a>,
	<a>Le Zhuo</a>,
        <strong>Shitian Zhao</strong>,
        <a>Siyuan Huang</a>,
        <a>Junlin Xie</a>,
        <a>Yu Qiao</a>,
	<a>Peng Gao</a>,
	<a>Hongsheng Li</a>
        <br>
        <em>ICLR</em>, 2025
        <br>       
        <a href="https://arxiv.org/abs/2409.15278">arXiv</a> /
        <a href="https://github.com/AFeng-x/PixWizard">code</a>
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:35%;vertical-align:middle">
          <img src='images/Causal_CoG.png' width="250">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2312.06685">
          <papertitle>Causal-CoG: A Causal-Effect Look at Context Generation for Boosting Multi-modal Language Models</papertitle>
        </a>
        <br>
        <strong>Shitian Zhao</strong>,
        <a href="https://lizw14.github.io/">Zhuowan Li</a>,
        <a>Yadong Lu</a>,
        <a href="https://cs.jhu.edu/~ayuille/">Alan Yuille</a>,
	<a href="https://wangyan921.github.io/">Yan Wang</a>
        <br>
        <em>CVPR (Poster Highlight, top 2.8%)</em>, 2024
        <br>       
        <a href="https://arxiv.org/abs/2312.06685">arXiv</a> /
        <a href="https://github.com/zhaoshitian/Causal-CoG">code</a>
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:35%;vertical-align:middle">
          <img src='images/lumina_mgpt.png' width="250">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2402.05935">
          <papertitle>Lumina-mGPT: Illuminate Flexible Photorealistic Text-to-Image Generation with Multimodal Generative Pretraining</papertitle>
        </a>
        <br>
        <a>Dongyang Liu*</a>,
        <strong>Shitian Zhao*</strong>,
        <a>Le Zhuo*</a>,
        <a>Weifeng Lin*</a>,
	<a>Hongsheng Li</a>,
	<a>Yu Qiao</a>,
	<a>Peng Gao*</a>
        <br>
        Preprint
        <br>
        <a href="https://arxiv.org/abs/2408.02657">arXiv</a> /
        <a href="https://github.com/Alpha-VLLM/Lumina-mGPT">code</a>
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:35%;vertical-align:middle">
          <img src='images/sphinx_x.png' width="250">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2402.05935">
          <papertitle>SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models</papertitle>
        </a>
        <br>
        <a>Dongyang Liu*</a>,
        <a>Longtian Qiu*</a>,
        <a>Siyuan Huang*</a>,
        <a>Weifeng Lin*</a>,
	<strong>Shitian Zhao</strong>,
	<a>Shijie Geng</a>,
	<a>Ziyi Lin</a>,
	<a>Peng Jin</a>,
	<a>Kaipeng Zhang</a>,
	<a>Wenqi Shao</a>,
	<a>Chao Xu</a>,
	<a>Conghui He</a>,
	<a>Junjun He</a>,
	<a>Hao Shao</a>,
	<a>Pan Lu</a>,
	<a>Hongsheng Li</a>,
	<a>Yu Qiao</a>,
	<a>Peng Gao*</a>
        <br>
        <em>ICML</em>, 2024
        <br>
        <a href="https://arxiv.org/abs/2402.05935">arXiv</a> /
        <a href="https://github.com/Alpha-VLLM/LLaMA2-Accessory">code</a>
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:35%;vertical-align:middle">
          <img src='images/likelihood_composition.png' width="250">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2312.06685">
          <papertitle>Unleashing the Potentials of Likelihood Composition for Multi-modal Language Models</papertitle>
        </a>
        <br>
        <strong>Shitian Zhao</strong>,
        <a>Renrui Zhang</a>,
        <a>Xu Luo</a>,
        <a>Yan Wang</a>,
	<a>Shanghang Zhang</a>,
	<a>Peng Gao</a>
        <br>
        <em>EMNLP Findings</em>, 2024
        <br> 
        <a href="https://arxiv.org/abs/2410.00363">arXiv</a> /
        <a href="https://github.com/zhaoshitian/Likelihood-Composition-Toolkit">code</a>
        <p></p>
      </td>
    </tr>

<!--     <tr>
      <td style="padding:20px;width:35%;vertical-align:middle">
          <img src='images/sphinx_and_anubis.png' width="250">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2312.06685">
          <papertitle>SPHINX and ANUBIS: Alleviating Deterministic and Generative Hallucinations of Multi-modal Large Language Models</papertitle>
        </a>
        <br>
        <strong>Shitian Zhao</strong>,
        <a>Han Xiao</a>,
        <a>Le Zhuo</a>,
        <a>Xu Luo</a>,
	<a>Hongsheng Li</a>,
	<a>Yu Qiao</a>,
	<a>Xiangyu Yue</a>,
	<a>Peng Gao</a>
        <br>
        <em>Submitted</em>, 2024
        <br>       
        <p></p>
      </td>
    </tr> -->

    <tr>
      <td style="padding:20px;width:35%;vertical-align:middle">
          <img src='images/image_e.jpeg' width="250">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2501.13920">
          <papertitle>IMAGINE-E: Image Generation Intelligence Evaluation of State-of-the-art Text-to-Image Models</papertitle>
        </a>
        <br>
	<a>Jiayi Lei*</a>,
	<a>Renrui Zhang*</a>,
	<a>Xiangfei Hu</a>,
	<a>Weifeng Lin</a>,
	<a>Zhen Li</a>,
	<a>Wenjian Sun</a>,
	<a>Ruoyi Du</a>,
	<a>Le Zhuo</a>,
	<a>Zhongyu Li</a>,
	<a>Xinyue Li</a>,
        <strong>Shitian Zhao</strong>,
	<a>Ziyu Guo</a>,
	<a>Yiting Lu</a>,
	<a>Peng Gao</a>,
        <a>Hongsheng Li</a>
        <br>
        <em>Preprint</em>, 2024
        <br>      
        <a href="https://arxiv.org/abs/2501.13920">arXiv</a>
        <p></p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:35%;vertical-align:middle">
          <img src='images/cl.jpeg' width="250">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2408.09984">
          <papertitle>Boosting Open-Domain Continual Learning via Leveraging Intra-domain Category-aware Prototype</papertitle>
        </a>
        <br>
	<a>Yadong Lu</a>,
        <strong>Shitian Zhao</strong>,
	<a>Boxiang Yun</a>,
	<a>Dongsheng Jiang</a>,
	<a>Yin Li</a>,
        <a>Qingli Li</a>,
	<a href="https://wangyan921.github.io/">Yan Wang</a>
        <br>
        <em>Preprint</em>, 2024
        <br>      
        <a href="https://arxiv.org/abs/2408.09984">arXiv</a>
        <p></p>
      </td>
    </tr>


<!--     <tr>
      <td style="padding:20px;width:35%;vertical-align:middle">
          <img src='images/vqcoop.png' width="250">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2312.06685">
          <papertitle>CLIP with Vector-Quantized Prompts for Continual Learning</papertitle>
        </a>
        <br>
        <strong>Shitian Zhao</strong>,
        <a>Yadong Lu</a>,
        <a>Xingran Xie</a>,
        <a>Qingli Li</a>,
	<a href="https://wangyan921.github.io/">Yan Wang</a>
        <br>
        <em>Submitted</em>, 2023
        <br>       
        <p></p>
      </td>
    </tr> -->

<!--     <tr>
      <td style="padding:20px;width:35%;vertical-align:middle">
          <img src='images/causal_seg.png' width="250">
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2312.06685">
          <papertitle>Prompting Universal Model for CT Image Segmentation from a Causal Perspective</papertitle>
        </a>
        <br>
	<a>Boxiang Yun</a>,
        <strong>Shitian Zhao</strong>,
        <a>Qingli Li</a>,
        <a>Alex Kot</a>,
	<a href="https://wangyan921.github.io/">Yan Wang</a>
        <br>
        <em>Submitted</em>, 2024
        <br>       
        <p></p>
      </td>
    </tr> -->
					
					
    </tbody></table>

        <hr>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2>Professional Service</h2>
		      <ul>
			      <li>Reviewer at NeurIPS2024, CVPR2025.</li>
		      </ul>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <hr>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2>Talks & Presentations</h2>
		    <ul>
		      <li><a href="https://docs.google.com/presentation/d/1XBtl9bWhFq1olp5QpK4Ka35ohQuhDP8nBQFbrOeUqyY/edit?usp=drive_link">Causal-CoG: A Causal-Effect Look at Context Generation for Boosting Multi-modal Language Models</a>, Group meeting @ CCVL, Johns Hopkins University</li>
		      <li><a href="https://n8c4mpqf19.feishu.cn/slides/CySHsjM69l8rxfd1x1wcSAyWnch?from=from_copylink">Lumina-mGPT: Illuminate Flexible Photorealistic Text-to-Image Generation with Multimodal Generative Pretraining</a>, Group meeting @ CCVL, Johns Hopkins University</li>
		    </ul>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <hr>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2>Honors & Awards</h2>
		      <ul>
			      <li>Outstanding Graduate Thesis @ ECNU</li>
			      <li>Excellent Student Scholarship (the first year in my undergraduate journey)</li>
			      <li>The Third Prize, Province Level, China Undergraduate Mathematical Contest in Modeling</li>
		      </ul>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <hr>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2>Misc</h2>
		      <ul>
			      <li>I used to be an actor @ Yang Zhi Shui Chinese Drama Club, ECNU. Our drama, <a href="https://www.thepaper.cn/newsDetail_forward_19296186">"Online Tragedy" (《线上悲剧》)</a>, won the Best Online Creative Award at The 18th Shanghai College Students Drama Festival. In the play, my role is the male lead, Andrei (安德烈), a Russian revolutionary youth. </li>
			      <li>During the summer of 2021, I joined the ecology research team from ECNU on multiple field investigations to Zhoushan Island in Zhejiang Province, primarily focusing on studying the distribution of local anteater populations. <a href="https://www.ecnu.edu.cn/info/1094/67863.htm">Finally, this research is published as a paper.</a></li>
		      </ul>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

	  <!--      足迹统计		-->
	  <div id="footer">
	    <div id="footer-text"></div>
	  </div>
	  © Shitian Zhao
	
	  <div class="container">
	    <div style="display:inline-block;width:600px;">
	      <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=sBNAoOp0z7WjCjePyIbth7sECqthsXr6e_SQ4G_VBhA&cl=ffffff&w=a"></script>
	    </div>
	  </div>

    </tbody></table>
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
        <td style="padding:0px">
          <br>
          <p style="text-align:right;font-size:small;">
            Website theme stolen from <a href="https://jonbarron.info/">Jon Barron</a>.
          </p>
        </td>
      </tr>
    </tbody></table>

      </td>
    </tr>
  </table>
</body>

</html>
